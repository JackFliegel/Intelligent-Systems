{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=8>HW4 - Intelligent Systems\\\n",
    "Question 2\\\n",
    "Bekarys Dukenbaev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HW3_datafiles/MNISTnumImages5000_balanced.txt\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "with open(\"HW3_datafiles/MNISTnumLabels5000_balanced.txt\") as f:\n",
    "    labels = f.readlines()\n",
    "\n",
    "labels = [x[0] for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('HW3_datafiles/MNISTnumImages5000_balanced.txt', sep='\\t', header=None)\n",
    "label_df = pd.read_csv('HW3_datafiles/MNISTnumLabels5000_balanced.txt', names=['label'])\n",
    "data_df['label'] = label_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "for i in range(10):  \n",
    "    digit_data = data_df[data_df['label'] == i]\n",
    "    digit_train = digit_data.sample(n=400, replace=False, random_state=42)\n",
    "    digit_test = digit_data[~digit_data.index.isin(digit_train.index)]\n",
    "\n",
    "    train_dfs.append(digit_train)\n",
    "    test_dfs.append(digit_test)\n",
    "    \n",
    "train_df = pd.concat(train_dfs, axis=0)\n",
    "test_df = pd.concat(test_dfs, axis=0)\n",
    "\n",
    "train_labels = train_df['label']\n",
    "test_labels = test_df['label']\n",
    "train_images = train_df.drop(columns=['label'])\n",
    "test_image = test_df.drop(columns=['label'])\n",
    "train_labels_onehot = pd.get_dummies(train_labels).values\n",
    "test_labels_onehot = pd.get_dummies(test_labels).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, learning_rate, beta):\n",
    "        self.W1, self.b1, self.W2, self.b2 = self.initialize_weights(input_size, hidden_size)\n",
    "        self.V_dW1, self.V_db1, self.V_dW2, self.V_db2 = self.initialize_momentum(input_size, hidden_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        \n",
    "    def initialize_weights(self, input_size, hidden_size):\n",
    "        W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        b1 = np.zeros((hidden_size, 1))\n",
    "        W2 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        b2 = np.zeros((input_size, 1))\n",
    "        return W1, b1, W2, b2\n",
    "\n",
    "    def initialize_momentum(self, input_size, hidden_size):\n",
    "        V_dW1 = np.zeros((hidden_size, input_size))\n",
    "        V_db1 = np.zeros((hidden_size, 1))\n",
    "        V_dW2 = np.zeros((input_size, hidden_size))\n",
    "        V_db2 = np.zeros((input_size, 1))\n",
    "        return V_dW1, V_db1, V_dW2, V_db2\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return 0.5 * np.mean(np.sum(np.square(y_true - y_pred), axis=0))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        Z1 = np.dot(self.W1, X) + self.b1\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        A2 = self.sigmoid(Z2)\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def backward_propagation(self, X, Y, Z1, A1, Z2, A2):\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = np.dot(dZ2, A1.T) / m\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * self.relu_derivative(Z1)\n",
    "        dW1 = np.dot(dZ1, X.T) / m\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "        \n",
    "        # Apply momentum\n",
    "        self.V_dW1 = self.beta * self.V_dW1 + (1 - self.beta) * dW1\n",
    "        self.V_db1 = self.beta * self.V_db1 + (1 - self.beta) * db1\n",
    "        self.V_dW2 = self.beta * self.V_dW2 + (1 - self.beta) * dW2\n",
    "        self.V_db2 = self.beta * self.V_db2 + (1 - self.beta) * db2\n",
    "        \n",
    "        # Update weights\n",
    "        self.W1 -= self.learning_rate * self.V_dW1\n",
    "        self.b1 -= self.learning_rate * self.V_db1\n",
    "        self.W2 -= self.learning_rate * self.V_dW2\n",
    "        self.b2 -= self.learning_rate * self.V_db2\n",
    "    \n",
    "    def train(self, X, Y, X_test, Y_test, epochs):\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        X_test = np.array(X_test)\n",
    "        Y_test = np.array(Y_test)\n",
    "\n",
    "        training_losses = []\n",
    "        test_losses = []\n",
    "        epochs_recorded = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            Z1, A1, Z2, A2_train = self.forward_propagation(X)\n",
    "            cost_train = self.compute_loss(Y, A2_train)\n",
    "            self.backward_propagation(X, Y, Z1, A1, Z2, A2_train)\n",
    "\n",
    "            _, _, _, A2_test = self.forward_propagation(X_test)\n",
    "            cost_test = self.compute_loss(Y_test, A2_test)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                epochs_recorded.append(epoch)\n",
    "                training_losses.append(cost_train)\n",
    "                test_losses.append(cost_test)\n",
    "                print(f'Epoch: {epoch}, Training Loss: {cost_train:.4f}, Test Loss: {cost_test:.4f}')\n",
    "                \n",
    "        return training_losses, test_losses, epochs_recorded\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        _, _, _, A2 = self.forward_propagation(X)\n",
    "        return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "input_size = 784  \n",
    "hidden_size = 150\n",
    "learning_rate = 0.1\n",
    "beta = 0.9\n",
    "autoencoder = Autoencoder(input_size, hidden_size, learning_rate, beta)\n",
    "\n",
    "X_train = train_images.T  \n",
    "X_test = test_image.T   \n",
    "\n",
    "# Train the autoencoder\n",
    "epochs = 250\n",
    "results = autoencoder.train(X_train, X_train, X_test, X_test, epochs)\n",
    "\n",
    "# Use the autoencoder to reconstruct the test images\n",
    "reconstructed_images = autoencoder.reconstruct(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[2], results[0], label='Training Loss')\n",
    "plt.plot(results[2], results[1], label='Test Loss')\n",
    "plt.title('Training and Test Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test latent space representation \n",
    "(not required, just out of curiosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_images = []\n",
    "for i in range(10):\n",
    "    digit_test_images = test_image[test_labels == i]\n",
    "    if not digit_test_images.empty:\n",
    "        first_image = digit_test_images.iloc[0].values.reshape(-1, 1)\n",
    "        reconstructed_image = autoencoder.reconstruct(first_image)\n",
    "        reconstructed_images.append(reconstructed_image)\n",
    "\n",
    "height = 28  \n",
    "width = 28   \n",
    "\n",
    "fig, axes = plt.subplots(10, 2, figsize=(10, 20)) \n",
    "\n",
    "for i, reconstructed_image in enumerate(reconstructed_images):\n",
    "    original_image = test_image[test_labels == i].iloc[0].values.reshape(height, width).T\n",
    "    reconstructed_image_reshaped = reconstructed_image.reshape(height, width).T\n",
    "\n",
    "    ax = axes[i, 0]\n",
    "    ax.imshow(original_image, cmap='gray')\n",
    "    ax.set_title(f\"Original {i}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = axes[i, 1]\n",
    "    ax.imshow(reconstructed_image_reshaped, cmap='gray')\n",
    "    ax.set_title(f\"Reconstructed {i}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulize a hidden neuron's feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hidden_neuron_feature(W1, neuron_index, subplot_index):\n",
    "    neuron_weights = W1[neuron_index, :]\n",
    "\n",
    "    neuron_image = neuron_weights.reshape(28, 28)\n",
    "\n",
    "    # Normalize the weights to the range [0, 255] if they aren't already\n",
    "    neuron_image -= neuron_image.min()  # translate so the minimum value is 0\n",
    "    neuron_image /= neuron_image.max()  # scale the values to the range [0, 1]\n",
    "    neuron_image *= 255  # scale the values to the range [0, 255]\n",
    "    \n",
    "    # Cast to an integer data type\n",
    "    neuron_image = neuron_image.astype(np.uint8)\n",
    "\n",
    "    # Display the image\n",
    "    plt.subplot(5, 4, subplot_index)\n",
    "    plt.imshow(neuron_image, cmap='gray')\n",
    "    plt.title(f\"Feature for hidden neuron {neuron_index}\")\n",
    "\n",
    "    # # Plot the activation of the neuron as a subplot\n",
    "    # plt.subplot(5, 4, subplot_index)\n",
    "    # plt.plot(neuron_weights)\n",
    "    # plt.title(f'Neuron {neuron_index}')\n",
    "    # plt.xlabel('Input Feature Index')\n",
    "    # plt.ylabel('Activation')\n",
    "\n",
    "neuron_indices = [147, 117, 29, 99, 67, 141, 89, 115, 53, 59, 130, 134, 9, 21, 142, 43, 15, 75, 139, 120]\n",
    "\n",
    "# # Specify which neuron to visualize\n",
    "# for neuron in neuron_indices:\n",
    "#     visualize_hidden_neuron_feature(autoencoder.W1, neuron_index=neuron)\n",
    "# # visualize_hidden_neuron_feature(autoencoder.W1, neuron_index=100)\n",
    "\n",
    "# Create a 5x4 grid of subplots for the 20 neurons\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, neuron_index in enumerate(neuron_indices, 1):\n",
    "    visualize_hidden_neuron_feature(autoencoder.W1, neuron_index, i)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
